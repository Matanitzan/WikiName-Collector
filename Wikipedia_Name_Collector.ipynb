{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZw+SQF2MoEeQT+fb+zZ5L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cfMq59BIAEKP"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import pandas as pd\n","from google.colab import files\n","\n","def get_wikidata_id(name):\n","# function to retrieve Wikidata ID for a given name\n","    api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={name}&prop=pageprops&format=json'\n","    response = requests.get(api_url).json()\n","    try:\n","        page_id = next(iter(response['query']['pages'].values()))['pageprops']['wikibase_item']\n","        return page_id\n","    except:\n","        return None\n","\n","def get_wikidata_info(wikidata_id,data_names):\n","# function to retrieve Wikidata information for a given Wikidata ID and add it to a list of names with information\n","  url = f\"https://www.wikidata.org/wiki/{wikidata_id}\"\n","  html = requests.get(url).content.decode('utf-8')\n","  soup = BeautifulSoup(html, 'html.parser')\n","  l = soup.findAll('a')\n","# find the name of the entry\n","  name=soup.findAll('span',{'class':'wikibase-title-label'})\n","# find the Wikidata ID of the entry\n","  wikidataID=soup.findAll('span',{'class':'wikibase-title-id'})\n","  nameN=name[0].text\n","# find the English description of the entry\n","  discrip=soup.findAll(\"span\",{'class':'wikibase-descriptionview-text'})\n","  description=discrip[0].text\n","# Find the div element that contains the sitelinks for the Wikidata item\n","  entries_div=soup.findAll(\"div\",{\"class\":\"wikibase-sitelinkgroupview\"})\n","  entries=entries_div[0].findAll(\"li\",{\"class\":\"wikibase-sitelinkview\"})\n","\n","  for i in entries:\n","# Loop through each sitelink list item and extract its language shortcut and linked page name\n","    lang=i.findAll(\"span\",{'class':'wikibase-sitelinkview-siteid'})\n","    wiki=lang[0].text.index('wiki')\n","    lang_shortcut=lang[0].text[:wiki]\n","    \n","    \n","    nameinlang=i.findAll('a')\n","    \n","    result=(nameN,wikidataID[0].text,description,lang_shortcut,lang[0].get('title'),nameinlang[0].text)\n","    data_names.append(result)\n","\n","\n","def init_full_name(NamePages):\n","# Create the URL for the given category\n","  url = f'https://en.wikipedia.org/wiki/Category:{NamePages}'\n","  names_with_info=[]\n","\n","  while True:\n","# Loop through all pages in the category\n","      response = requests.get(url)\n","      html_content = response.text\n","      soup = BeautifulSoup(html_content, 'html.parser')\n","# Get all name links on the page\n","      name_links = soup.select('.mw-category.mw-category-columns a')\n","# Filter out links that are not relevant\n","      if NamePages==\"Given_names\":\n","        filtered_links = [link for link in name_links if not any(substring in link['href'] for substring in ['/wiki/Given_name', '/wiki/Template:R_from_given_name', '/wiki/List_of_most_popular_given_names', '/wiki/Onomancy']) and not link.find_parent(attrs={'id': 'mw-subcategories'})]\n","      if NamePages==\"Surnames\":\n","        filtered_links = [link for link in name_links if not any(substring in link['href'] for substring in ['/wiki/Name_blending', '/wiki/One-name_study', '/wiki/Template:R_from_surname', '/wiki/Template:Surname']) and not link.find_parent(attrs={'id': 'mw-subcategories'})]\n","      for link in filtered_links:\n","        full_name = link.text.strip()\n","        wikidata_info = get_wikidata_id(full_name)\n","        if wikidata_info:\n","# Get additional information about the name from Wikidata\n","          get_wikidata_info(wikidata_info, names_with_info)\n","\n","# Check if there is a link to the next page\n","      next_link = soup.find('a', text='next page')\n","      if not next_link:\n","          break\n","\n","      # Update the URL to the next page\n","      url = 'https://en.wikipedia.org' + next_link['href']\n","\n","# Create a Pandas DataFrame from the list of names with information\n","  results_df = pd.DataFrame(names_with_info, columns=['Label', 'WikiData ID', 'English Description', 'Language','Wiki ShortLang','Entry'])\n","  results_df.to_csv('surname.csv', index=False, encoding='utf-8-sig')\n","  files.download('surname.csv')\n","  # results_df.to_csv('name.csv', index=False, encoding='utf-8-sig')\n","  # files.download('name.csv')\n","\n","# NamePages=\"Given_names\"\n","NamePages1=\"Surnames\"\n","# init_full_name(NamePages)\n","init_full_name(NamePages1)"]}]}